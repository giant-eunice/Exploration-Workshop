{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration_SSAC 01 \n",
    "## 가위바위보 분류기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류기 생성 과정\n",
    "    * 데이터 준비 -> 딥러닝 네트워크 설계 -> 학습 -> 테스트(평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정확도를 올리기 위한 방법  \n",
    "1) 데이터 양 늘리기  \n",
    "2) Data Augmentation (데이터를 변형)  \n",
    "3) Layer structure  \n",
    "4) HyperParameter 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 필요한 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIL 라이브러리 :  파이썬 인터프리터에 다양한 이미지 파일 형식을 지원하고 강력한 이미지 처리와 그래픽 기능을 제공하는 자유-오픈 소스 소프트웨어 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./Downloads/exit/envs/aiffel/lib/python3.7/site-packages (8.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train 데이터 준비 - Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 직접 생성한 300장 + 팀원 데이터 2100장 + 구글 데이터 2520장 + 기타 팀원 데이터 600장\n",
    "* 중복 파일명 수정 과정에서 일단 stop! (파이썬 문법이 약하니 겪게 되는 문제 ㅠㅠ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def changeName(path, cName):\n",
    "#     i = 0\n",
    "#     for filename in os.listdir(path):\n",
    "#         print(path+filename, '=>', path+str(cName)+str(i)+'.jpg')\n",
    "#         os.rename(path+filename, path+str(cName)+str(i)+'.jpg')\n",
    "#         i += 1\n",
    " \n",
    "# changeName('/home/ssac22/Downloads/rock_scissor_paper/paper','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ssac22'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train 데이터 준비 - 현승님 제공 Data Load\n",
    "* 클래스별 1100장 총 3300장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로: /home/ssac22/aiffel/train_total/scissor\n",
      "Train Image Resize 완료\n"
     ]
    }
   ],
   "source": [
    "# scissor 파일 안의 사진 크기 28X28 크기로 resize 진행 및 확인\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train_total/scissor\"\n",
    "print(\"이미지 디렉토리 경로:\", image_dir_path)\n",
    "\n",
    "images = glob.glob(image_dir_path+\"/*.jpg\")\n",
    "\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img = Image.open(img)\n",
    "    new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "    new_img.save(img, \"JPEG\")\n",
    "    \n",
    "print(\"Train Image Resize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로: /home/ssac22/aiffel/train_total/rock\n",
      "Train Image Resize 완료\n"
     ]
    }
   ],
   "source": [
    "# rock 파일 안의 사진 크기 28X28 크기로 resize 진행 및 확인\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train_total/rock\"\n",
    "print(\"이미지 디렉토리 경로:\", image_dir_path)\n",
    "\n",
    "images = glob.glob(image_dir_path+\"/*.jpg\")\n",
    "\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img = Image.open(img)\n",
    "    new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "    new_img.save(img, \"JPEG\")\n",
    "    \n",
    "print(\"Train Image Resize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로: /home/ssac22/aiffel/train_total/paper\n",
      "Train Image Resize 완료\n"
     ]
    }
   ],
   "source": [
    "# paper 파일 안의 사진 크기 28X28 크기로 resize 진행 및 확인\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train_total/paper\"\n",
    "print(\"이미지 디렉토리 경로:\", image_dir_path)\n",
    "\n",
    "images = glob.glob(image_dir_path+\"/*.jpg\")\n",
    "\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img = Image.open(img)\n",
    "    new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "    new_img.save(img, \"JPEG\")\n",
    "    \n",
    "print(\"Train Image Resize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 3300 입니다.\n",
      "x_train shape: (3300, 28, 28, 3)\n",
      "y_train shape: (3300,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터로드에 활용될 메서드 load_data 정의\n",
    "def load_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=3300   # 가위바위보 이미지 개수 총 개수\n",
    "    img_size=28 # 이미지 크기\n",
    "    color=3 # 컬러 사진이기 때문에 RGB channel 3 :: 흑백일 때는 1\n",
    "    \n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0 # 초기값 설정\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "# 이미지 경로를 인자로 넣으면, 각 경로에 있는 이미지를 행렬 영역을 생성하고, \n",
    "# 각 항목에 맞는 label을 부여해 줘서 최종적으로 데이터 / 라벨로 return 해줌\n",
    "\n",
    "# 정의내린 함수를 활용하여 데이터를 로드해 보자\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train_total\" # 경로 체크 중요\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화 :: 값의 범위를 줄여 분포를 좁게 만들면 정확도도 올라갈 것이다\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train_norm.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Check**  \n",
    "\n",
    "* **could not broadcast input array from shape (224,224,3) into shape (28,28,3)**    \n",
    "데이터 로드 과정에서 만났던 error인데, 처음에는 어떤 의미인지 감을 못 잡아 오래 헤맸는데,  \n",
    "input shape 에 데이터를 넣는 과정에서 문제가 생겼다고 판단하여,  \n",
    "다시 한번 데이터 로드 과정 전에 가위, 바위, 보의 각 파일에 속한 이미지들이 resize되었는지 확인해 보니 안 된 부분이 있었다..!   \n",
    "다시 resize하여 사이즈를 맞춰줬더니 정상적으로 동작!  \n",
    "\n",
    "\n",
    "* broadcast 관련 error를 마주친다면,  \n",
    "설정해 준 input size, input shape과 data_load 과정에서 직접 input 되는 데이터들의 규격이 맞는지 다시 한번 더 확인하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ssac22/aiffel/train_total\n"
     ]
    }
   ],
   "source": [
    "print(image_dir_path) # 자꾸 오류가 떠서 경로 문제일까 싶어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_norm[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 딥러닝 네트워크 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_46 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 114,963\n",
      "Trainable params: 114,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1=64\n",
    "n_channel_2=64\n",
    "n_channel_3=128\n",
    "n_channel_4=64\n",
    "n_dense=16\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "# model.add(keras.layers.Conv2D(n_channel_4, (3,3), activation='relu'))\n",
    "# model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Error Check  \n",
    "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_1/convolution' (op: Negative dimension size caused by subtracting 3 from 1 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](max_pooling2d_5/Identity, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [?,1,1,128], [3,3,128,256].\n",
    "\n",
    "\n",
    "초반에 깊은 고민없이, Layer와 Conv2D, MaxPooling을 추가하였더니,  \n",
    "Conv2D 연산이 되지 않는 수치에 도달했던 것 같다.  \n",
    "\n",
    "그래서 해당 레이어를 삭제하고 진행하였더니 오류는 없어졌다.  \n",
    "(이는, CNN Network에서 Convolutional 연산과 Maxpooling의 개념과 과정을 정확히 알고 있다면 일어나지 않을 실수일 것이다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1121 - accuracy: 0.9594\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0858 - accuracy: 0.9694\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0946 - accuracy: 0.9624\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0808 - accuracy: 0.9694\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 1s 12ms/step - loss: 0.0578 - accuracy: 0.9794\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0551 - accuracy: 0.9818\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0669 - accuracy: 0.9755\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0610 - accuracy: 0.9797\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0623 - accuracy: 0.9800\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0945 - accuracy: 0.9621\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0718 - accuracy: 0.9755\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0589 - accuracy: 0.9797\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0451 - accuracy: 0.9855\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0509 - accuracy: 0.9818\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0640 - accuracy: 0.9776\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0364 - accuracy: 0.9882\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0294 - accuracy: 0.9885\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 0.9903\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0492 - accuracy: 0.9848\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.0418 - accuracy: 0.9876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5b03f0650>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train_epoch=20\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Data 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로:  /home/ssac22/aiffel/rps_test/paper\n",
      "보 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/ssac22/aiffel/rps_test/rock\n",
      "주먹 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/ssac22/aiffel/rps_test/scissor\n",
      "가위 이미지 resize 완료!\n",
      "(28, 28)\n",
      "테스트데이터(x_test)의 이미지 개수는 2520 입니다.\n",
      "x_test shape: (2520, 28, 28, 3)\n",
      "y_test shape: (2520,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Data Load를 위한 사전 준비\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rps_test/paper\" # 경로 주의\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "images=glob.glob(image_dir_path + \"/*.png\") # 사용한 구글 이미지 데이터 확장자!! check!\n",
    "target_size=(28,28) # resize check\n",
    "\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img=new_img.convert('RGB')\n",
    "    new_img.save(img,\"png\")\n",
    "print(\"보 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rps_test/rock\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "images=glob.glob(image_dir_path + \"/*.png\")  \n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img=new_img.convert('RGB')\n",
    "    new_img.save(img,\"png\")\n",
    "print(\"주먹 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rps_test/scissor\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "images=glob.glob(image_dir_path + \"/*.png\")  \n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img=new_img.convert('RGB')\n",
    "    new_img.save(img,\"png\")\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "print(new_img.size)\n",
    "\n",
    "\n",
    "# load_data 함수 정의\n",
    "def load_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=2520   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.png'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.png'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.png'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "# 실질적 load_data\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rps_test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 테스트 데이터를 활용하여 준비된 딥러닝 네트워크 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 - 0s - loss: 1.8218 - accuracy: 0.7067\n",
      "test_loss: 1.8218151330947876 \n",
      "test_accuracy: 0.7067460417747498\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) teachable 사이트를 활용하여 빠르게 데이터를 직접 촬영하고 저장할 수 있음  \n",
    "2) 직접 촬영한 데이터 300개를 가지고 학습시키고 다른 데이터를 활용하여 test했을 때, 30% 남짓한 정확도를 얻음  \n",
    "3) 1단계로 데이터 부족이 큰 요소라 생각하여, 동료들의 데이터를 모았고, (파일명이 겹치는 문제를 해결하지 못해, 다른 데이터 사용)  \n",
    "4) 기존의 224 * 224 크기의 이미지를 28 * 28 사이즈로 줄여주고, (input size 조절)  \n",
    "5) load_data 함수를 정의하여, 각 경로에 맞는 데이터를 로드 완료  \n",
    "6) keras를 활용한 Deep Learning Network를 만들었고,  \n",
    "7) 준비된 데이터로 학습을 시킨 다음  \n",
    "8) test용 데이터를 따로 준비하여 전처리 및 정규화 진행  \n",
    "9) 만들어 놓은 모델을 사용하여 test!  \n",
    "10) 학습하고 평가하여 최종 정확도 70% 도달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 문법 및 데이터 크롤링을 통하여 더 많은 사전 데이터를 수집하고, 전처리를 진행하였으면 정확도가 더 개선되지 않았을까 싶다.  \n",
    "또한, 네트워크 구조 이해를 통해 더 정밀한 네트워크를 설계하는 것이 중요했고,  \n",
    "데이터를 추가적으로 수집하는 Offline Data Augmentation이 아닌, 랜덤으로 변형하여 들어가는 Online Data Augmentation을 적용해 보면 더 개선될 것 같다.  \n",
    "\n",
    "마지막으로, 기억해 두어야 할 부분은 **데이터 수집 -> 데이터 전처리 -> 네트워크 설계 -> 학습 -> 평가** 의 Process를 잊지 말자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
